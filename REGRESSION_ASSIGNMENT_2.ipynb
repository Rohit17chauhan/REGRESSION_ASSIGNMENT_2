{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc46e4b8",
   "metadata": {},
   "source": [
    "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it\n",
    "represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e77b1fe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'R-squared (also known as the coefficient of determination) is a statistical measure used to evaluate the goodness-of-fit\\nof a linear regression model. It represents the proportion of the variance in the dependent variable (response variable) that is\\npredictable from the independent variable(s) (predictor variables).\\nCalculation of R-squared:\\nR-squared is calculated using the following formula:\\n\\n                                  R^2=1-𝑆𝑆𝑟𝑒𝑠/SStot\\n                                  \\nWhere:\\n\\n\\u200b𝑆𝑆𝑟𝑒𝑠\\n  (Residual Sum of Squares): The sum of the squared differences between the actual values and the predicted values from the\\n  regression model.\\n  \\n  \\n\\u200bSStot\\n  (Total Sum of Squares): The sum of the squared differences between the actual values and the mean of the dependent variable.\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ans1=\"\"\"R-squared (also known as the coefficient of determination) is a statistical measure used to evaluate the goodness-of-fit\n",
    "of a linear regression model. It represents the proportion of the variance in the dependent variable (response variable) that is\n",
    "predictable from the independent variable(s) (predictor variables).\n",
    "Calculation of R-squared:\n",
    "R-squared is calculated using the following formula:\n",
    "\n",
    "                                  R^2=1-𝑆𝑆𝑟𝑒𝑠/SStot\n",
    "                                  \n",
    "Where:\n",
    "\n",
    "​𝑆𝑆𝑟𝑒𝑠\n",
    "  (Residual Sum of Squares): The sum of the squared differences between the actual values and the predicted values from the\n",
    "  regression model.\n",
    "  \n",
    "  \n",
    "​SStot\n",
    "  (Total Sum of Squares): The sum of the squared differences between the actual values and the mean of the dependent variable.\n",
    "\n",
    "\"\"\"\n",
    "Ans1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40099dc5",
   "metadata": {},
   "source": [
    "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "476f0740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Adjusted R-squared is a modified version of the regular R-squared that accounts for the number of predictors in the \\nmodel. Unlike regular R-squared, which can increase whenever a new predictor is added (regardless of whether the predictor is \\nmeaningful), adjusted R-squared adjusts for the number of predictors and only increases when the new predictor improves the\\nmodel more than would be expected by chance.\\n\\nHow Adjusted R-squared Differs from R-squared:\\nR-squared always increases or stays the same when additional predictors are added, even if the predictors are irrelevant. \\nThis can give a false sense of improvement in the model.\\n\\nAdjusted R-squared accounts for the number of predictors by penalizing models with unnecessary predictors. It only increases\\nwhen the new variable improves the model significantly. If the added variable doesn’t improve the model, adjusted R-squared can\\nactually decrease.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ans2=\"\"\"Adjusted R-squared is a modified version of the regular R-squared that accounts for the number of predictors in the \n",
    "model. Unlike regular R-squared, which can increase whenever a new predictor is added (regardless of whether the predictor is \n",
    "meaningful), adjusted R-squared adjusts for the number of predictors and only increases when the new predictor improves the\n",
    "model more than would be expected by chance.\n",
    "\n",
    "How Adjusted R-squared Differs from R-squared:\n",
    "R-squared always increases or stays the same when additional predictors are added, even if the predictors are irrelevant. \n",
    "This can give a false sense of improvement in the model.\n",
    "\n",
    "Adjusted R-squared accounts for the number of predictors by penalizing models with unnecessary predictors. It only increases\n",
    "when the new variable improves the model significantly. If the added variable doesn’t improve the model, adjusted R-squared can\n",
    "actually decrease.\"\"\"\n",
    "Ans2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c228f6",
   "metadata": {},
   "source": [
    "Q3. When is it more appropriate to use adjusted R-squared?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d062c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Adjusted R-squared is more appropriate to use than regular R-squared in the following situations:\\n\\n1. When Comparing Models with Different Numbers of Predictors\\nIf you're comparing models with different numbers of independent variables (predictors), adjusted R-squared provides a fairer \\ncomparison. Regular R-squared tends to increase as more predictors are added, even if those predictors don't improve the model \\nsignificantly. Adjusted R-squared accounts for this and penalizes for unnecessary complexity. It’s useful when deciding if \\nadding a new variable truly improves the model.\\n\\n2. To Avoid Overfitting\\nAdjusted R-squared is helpful in identifying whether your model is overfitting the data by including too many predictors. \\nOverfitting occurs when a model is too complex, capturing noise rather than the true underlying relationship. A high R-squared \\nmight indicate overfitting, while adjusted R-squared will decline if irrelevant variables are added, signaling that the model's \\ncomplexity may not be justified.\\n\\n3. For Models with Multiple Predictors\\nIn regression models with multiple predictors, regular R-squared can be misleading because it doesn’t distinguish between\\npredictors that contribute meaningfully to the model and those that don’t. Adjusted R-squared corrects for the number of\\npredictors, giving a better indication of model quality when dealing with multivariate models.\\n\\n4. When You Want to Penalize Model Complexity\\nIf you aim to strike a balance between model simplicity and explanatory power, adjusted R-squared is appropriate because it\\npenalizes adding predictors that don't significantly improve the model. It helps you prioritize parsimony—getting the best \\nexplanatory power with the fewest predictors.\\n\\n5. For Evaluating Stepwise or Backward Regression\\nIn procedures like stepwise regression or backward elimination, where variables are added or removed based on their statistical\\nsignificance, adjusted R-squared can be a useful criterion. It helps determine if adding or removing a variable truly improves \\nthe model beyond what would be expected by chance.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ans3=\"\"\"Adjusted R-squared is more appropriate to use than regular R-squared in the following situations:\n",
    "\n",
    "1. When Comparing Models with Different Numbers of Predictors\n",
    "If you're comparing models with different numbers of independent variables (predictors), adjusted R-squared provides a fairer \n",
    "comparison. Regular R-squared tends to increase as more predictors are added, even if those predictors don't improve the model \n",
    "significantly. Adjusted R-squared accounts for this and penalizes for unnecessary complexity. It’s useful when deciding if \n",
    "adding a new variable truly improves the model.\n",
    "\n",
    "2. To Avoid Overfitting\n",
    "Adjusted R-squared is helpful in identifying whether your model is overfitting the data by including too many predictors. \n",
    "Overfitting occurs when a model is too complex, capturing noise rather than the true underlying relationship. A high R-squared \n",
    "might indicate overfitting, while adjusted R-squared will decline if irrelevant variables are added, signaling that the model's \n",
    "complexity may not be justified.\n",
    "\n",
    "3. For Models with Multiple Predictors\n",
    "In regression models with multiple predictors, regular R-squared can be misleading because it doesn’t distinguish between\n",
    "predictors that contribute meaningfully to the model and those that don’t. Adjusted R-squared corrects for the number of\n",
    "predictors, giving a better indication of model quality when dealing with multivariate models.\n",
    "\n",
    "4. When You Want to Penalize Model Complexity\n",
    "If you aim to strike a balance between model simplicity and explanatory power, adjusted R-squared is appropriate because it\n",
    "penalizes adding predictors that don't significantly improve the model. It helps you prioritize parsimony—getting the best \n",
    "explanatory power with the fewest predictors.\n",
    "\n",
    "5. For Evaluating Stepwise or Backward Regression\n",
    "In procedures like stepwise regression or backward elimination, where variables are added or removed based on their statistical\n",
    "significance, adjusted R-squared can be a useful criterion. It helps determine if adding or removing a variable truly improves \n",
    "the model beyond what would be expected by chance.\"\"\"\n",
    "Ans3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1f3c20",
   "metadata": {},
   "source": [
    "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
    "calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a57ba4f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In regression analysis, RMSE, MSE, and MAE are error metrics used to evaluate the performance of a model by measuring\\nhow well its predictions align with the actual values. These metrics quantify the difference between the predicted and actual \\nvalues, providing insights into the model's accuracy.\\n\\n1. Mean Squared Error (MSE)\\nMSE is the average of the squared differences between the predicted values (ŷi)and the actual values(yi)in the dataset. \\nIt penalizes large errors more than small ones because the differences are squared, which magnifies the effect of larger errors.\\nFORMULA\\n  MSE=1/n(summation of(yi-ŷi)^2)\\n  Where:\\n\\nREPRESENTATION:\\nn is the number of observations (data points).\\n𝑦𝑖 is the actual value.\\nŷi is the predicted value.\\n\\n\\n\\n2. Root Mean Squared Error (RMSE)\\nRMSE is the square root of the MSE. It has the same unit as the dependent variable, making it easier to interpret in practical \\nterms. RMSE is useful when you want to measure the average error in the same units as the target variable, and it also\\npenalizes large errors like MSE.\\nFORMULA\\n RMSE=wholeroot(1/n(yi-ŷi)^2)\\n\\n3. Mean Absolute Error (MAE)\\nMAE is the average of the absolute differences between the predicted and actual values. Unlike MSE and RMSE, MAE doesn’t \\nsquare the errors, so it treats all errors linearly and doesn’t disproportionately penalize large errors.\\nFORMULA\\n MAE=1/n(|yi-ŷi|)\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ans4=\"\"\"In regression analysis, RMSE, MSE, and MAE are error metrics used to evaluate the performance of a model by measuring\n",
    "how well its predictions align with the actual values. These metrics quantify the difference between the predicted and actual \n",
    "values, providing insights into the model's accuracy.\n",
    "\n",
    "1. Mean Squared Error (MSE)\n",
    "MSE is the average of the squared differences between the predicted values (ŷi)and the actual values(yi)in the dataset. \n",
    "It penalizes large errors more than small ones because the differences are squared, which magnifies the effect of larger errors.\n",
    "FORMULA\n",
    "  MSE=1/n(summation of(yi-ŷi)^2)\n",
    "  Where:\n",
    "\n",
    "REPRESENTATION:\n",
    "n is the number of observations (data points).\n",
    "𝑦𝑖 is the actual value.\n",
    "ŷi is the predicted value.\n",
    "\n",
    "\n",
    "\n",
    "2. Root Mean Squared Error (RMSE)\n",
    "RMSE is the square root of the MSE. It has the same unit as the dependent variable, making it easier to interpret in practical \n",
    "terms. RMSE is useful when you want to measure the average error in the same units as the target variable, and it also\n",
    "penalizes large errors like MSE.\n",
    "FORMULA\n",
    " RMSE=wholeroot(1/n(yi-ŷi)^2)\n",
    "\n",
    "3. Mean Absolute Error (MAE)\n",
    "MAE is the average of the absolute differences between the predicted and actual values. Unlike MSE and RMSE, MAE doesn’t \n",
    "square the errors, so it treats all errors linearly and doesn’t disproportionately penalize large errors.\n",
    "FORMULA\n",
    " MAE=1/n(|yi-ŷi|)\n",
    "\"\"\"\n",
    "Ans4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2685facd",
   "metadata": {},
   "source": [
    "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
    "regression analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f97dbd7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Mean Squared Error (MSE)\\nAdvantages:\\nMathematically Convenient:\\n\\nSquaring the errors ensures that MSE is differentiable, making it suitable for optimization techniques like gradient descent, which are common in machine learning algorithms.\\nHeavily Penalizes Large Errors:\\n\\nBecause the errors are squared, larger errors contribute significantly more to the total error. This can be useful when large deviations from the actual values are particularly undesirable (e.g., safety-critical applications).\\nWidely Used in Statistical Models:\\n\\nMSE is a commonly used metric in statistical regression and machine learning, and it's simple to calculate and interpret when considering the squared loss.\\nDisadvantages:\\nSensitive to Outliers:\\nThe squaring of errors makes MSE very sensitive to outliers. A few large errors can disproportionately affect the metric, leading to a skewed perception of model performance.\\nNot in the Same Unit as the Dependent Variable:\\nSince MSE squares the errors, the metric is in squared units of the original response variable, making it harder to interpret directly in terms of the actual variable being predicted (e.g., squared dollars, squared meters, etc.).\\nRoot Mean Squared Error (RMSE)\\nAdvantages:\\nInterpretable in the Same Units as the Dependent Variable:\\n\\nUnlike MSE, RMSE has the same units as the response variable, making it easier to understand and communicate the average size of the error in practical terms (e.g., dollars, meters, etc.).\\nPenalizes Large Errors:\\n\\nLike MSE, RMSE penalizes large errors more due to the squaring process. This can be an advantage when large prediction errors need to be minimized, but the results still need to be interpretable.\\nWidely Used in Practice:\\n\\nRMSE is frequently used in applications where having an error metric in the same unit as the target variable is beneficial, making it intuitive to stakeholders.\\nDisadvantages:\\nSensitive to Outliers:\\nLike MSE, RMSE is sensitive to outliers due to the squaring of errors. A single large error can disproportionately affect the RMSE value, which can make it less robust in datasets with outliers.\\nPenalizes Large Errors More than Small Errors:\\nWhile this might be an advantage in certain contexts, it can also be a disadvantage if all errors (large and small) should be treated equally, leading to potentially over-emphasizing larger errors.\\nMean Absolute Error (MAE)\\nAdvantages:\\nLess Sensitive to Outliers:\\n\\nSince MAE calculates the absolute difference between predicted and actual values without squaring, it is less sensitive to outliers than MSE or RMSE. This makes it more robust when outliers are present in the data.\\nIntuitive and Interpretable:\\n\\nMAE represents the average absolute error, making it easy to interpret directly as the average magnitude of the prediction errors in the same unit as the response variable. For example, an MAE of 5 means the model’s predictions are off by 5 units on average.\\nAll Errors Treated Equally:\\n\\nMAE doesn’t disproportionately penalize large errors, treating all errors linearly. This is advantageous when the cost of all errors is the same and large deviations are not particularly concerning.\\nDisadvantages:\\nLess Emphasis on Large Errors:\\n\\nBecause MAE doesn’t square the errors, large errors are not penalized as much as they are with MSE or RMSE. This can be a disadvantage when large errors are especially costly or need to be minimized.\\nLess Useful in Optimization:\\n\\nMAE is not as mathematically convenient for optimization as MSE. The lack of a squared term means the gradient is not continuous, which can make it harder to optimize in some machine learning algorithms (e.g., gradient-based methods).\\nIgnores the Direction of Errors:\\n\\nMAE only considers the magnitude of the errors and not their direction (whether the predictions are higher or lower than the actual values), which might be a limitation in certain applications.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ans5=\"\"\"Mean Squared Error (MSE)\n",
    "Advantages:\n",
    "Mathematically Convenient:\n",
    "\n",
    "Squaring the errors ensures that MSE is differentiable, making it suitable for optimization techniques like gradient descent, which are common in machine learning algorithms.\n",
    "Heavily Penalizes Large Errors:\n",
    "\n",
    "Because the errors are squared, larger errors contribute significantly more to the total error. This can be useful when large deviations from the actual values are particularly undesirable (e.g., safety-critical applications).\n",
    "Widely Used in Statistical Models:\n",
    "\n",
    "MSE is a commonly used metric in statistical regression and machine learning, and it's simple to calculate and interpret when considering the squared loss.\n",
    "Disadvantages:\n",
    "Sensitive to Outliers:\n",
    "The squaring of errors makes MSE very sensitive to outliers. A few large errors can disproportionately affect the metric, leading to a skewed perception of model performance.\n",
    "Not in the Same Unit as the Dependent Variable:\n",
    "Since MSE squares the errors, the metric is in squared units of the original response variable, making it harder to interpret directly in terms of the actual variable being predicted (e.g., squared dollars, squared meters, etc.).\n",
    "Root Mean Squared Error (RMSE)\n",
    "Advantages:\n",
    "Interpretable in the Same Units as the Dependent Variable:\n",
    "\n",
    "Unlike MSE, RMSE has the same units as the response variable, making it easier to understand and communicate the average size of the error in practical terms (e.g., dollars, meters, etc.).\n",
    "Penalizes Large Errors:\n",
    "\n",
    "Like MSE, RMSE penalizes large errors more due to the squaring process. This can be an advantage when large prediction errors need to be minimized, but the results still need to be interpretable.\n",
    "Widely Used in Practice:\n",
    "\n",
    "RMSE is frequently used in applications where having an error metric in the same unit as the target variable is beneficial, making it intuitive to stakeholders.\n",
    "Disadvantages:\n",
    "Sensitive to Outliers:\n",
    "Like MSE, RMSE is sensitive to outliers due to the squaring of errors. A single large error can disproportionately affect the RMSE value, which can make it less robust in datasets with outliers.\n",
    "Penalizes Large Errors More than Small Errors:\n",
    "While this might be an advantage in certain contexts, it can also be a disadvantage if all errors (large and small) should be treated equally, leading to potentially over-emphasizing larger errors.\n",
    "Mean Absolute Error (MAE)\n",
    "Advantages:\n",
    "Less Sensitive to Outliers:\n",
    "\n",
    "Since MAE calculates the absolute difference between predicted and actual values without squaring, it is less sensitive to outliers than MSE or RMSE. This makes it more robust when outliers are present in the data.\n",
    "Intuitive and Interpretable:\n",
    "\n",
    "MAE represents the average absolute error, making it easy to interpret directly as the average magnitude of the prediction errors in the same unit as the response variable. For example, an MAE of 5 means the model’s predictions are off by 5 units on average.\n",
    "All Errors Treated Equally:\n",
    "\n",
    "MAE doesn’t disproportionately penalize large errors, treating all errors linearly. This is advantageous when the cost of all errors is the same and large deviations are not particularly concerning.\n",
    "Disadvantages:\n",
    "Less Emphasis on Large Errors:\n",
    "\n",
    "Because MAE doesn’t square the errors, large errors are not penalized as much as they are with MSE or RMSE. This can be a disadvantage when large errors are especially costly or need to be minimized.\n",
    "Less Useful in Optimization:\n",
    "\n",
    "MAE is not as mathematically convenient for optimization as MSE. The lack of a squared term means the gradient is not continuous, which can make it harder to optimize in some machine learning algorithms (e.g., gradient-based methods).\n",
    "Ignores the Direction of Errors:\n",
    "\n",
    "MAE only considers the magnitude of the errors and not their direction (whether the predictions are higher or lower than the actual values), which might be a limitation in certain applications.\"\"\"\n",
    "Ans5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62397a77",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is\n",
    "it more appropriate to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf3eb37c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lasso regularization (Least Absolute Shrinkage and Selection Operator) is a type of linear regression technique that\\nincorporates a regularization term to prevent overfitting by shrinking the coefficients of less important predictors toward \\nzero. This results in a simpler model with fewer variables and reduces the risk of overfitting, especially when there are many\\nfeatures.\\nKey Characteristics of Lasso:\\nL1 Norm Regularization: The penalty is based on the sum of the absolute values of the coefficients.\\nFeature Selection: Lasso can set some coefficients exactly to zero, which means it can automatically exclude irrelevant or \\nredundant features, making the model more interpretable and simpler.\\nSparse Solutions: Lasso tends to result in models with fewer non-zero coefficients, which is particularly useful in \\nhigh-dimensional datasets where many features might be irrelevant.\\n\\nRidge Regularization (L2 Regularization):\\nIn contrast to Lasso, Ridge regularization adds a penalty based on the L2 norm (the sum of the squared coefficients) to the\\nloss function:\\nUnlike Lasso, Ridge shrinks the coefficients, but it doesn’t drive them to exactly zero. This means Ridge regularization \\nshrinks all coefficients toward zero but retains all features in the model.\\n\\nKey Characteristics of Ridge:\\nL2 Norm Regularization: The penalty is based on the sum of the squared values of the coefficients.\\nNo Feature Selection: Ridge regression shrinks the coefficients of less important features, but it never eliminates them \\n(coefficients remain non-zero).\\nBetter for Multicollinearity: Ridge works well when there is high correlation among features (multicollinearity), as it\\ndistributes the coefficient values across correlated features instead of excluding them.\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ans6=\"\"\"Lasso regularization (Least Absolute Shrinkage and Selection Operator) is a type of linear regression technique that\n",
    "incorporates a regularization term to prevent overfitting by shrinking the coefficients of less important predictors toward \n",
    "zero. This results in a simpler model with fewer variables and reduces the risk of overfitting, especially when there are many\n",
    "features.\n",
    "Key Characteristics of Lasso:\n",
    "L1 Norm Regularization: The penalty is based on the sum of the absolute values of the coefficients.\n",
    "Feature Selection: Lasso can set some coefficients exactly to zero, which means it can automatically exclude irrelevant or \n",
    "redundant features, making the model more interpretable and simpler.\n",
    "Sparse Solutions: Lasso tends to result in models with fewer non-zero coefficients, which is particularly useful in \n",
    "high-dimensional datasets where many features might be irrelevant.\n",
    "\n",
    "Ridge Regularization (L2 Regularization):\n",
    "In contrast to Lasso, Ridge regularization adds a penalty based on the L2 norm (the sum of the squared coefficients) to the\n",
    "loss function:\n",
    "Unlike Lasso, Ridge shrinks the coefficients, but it doesn’t drive them to exactly zero. This means Ridge regularization \n",
    "shrinks all coefficients toward zero but retains all features in the model.\n",
    "\n",
    "Key Characteristics of Ridge:\n",
    "L2 Norm Regularization: The penalty is based on the sum of the squared values of the coefficients.\n",
    "No Feature Selection: Ridge regression shrinks the coefficients of less important features, but it never eliminates them \n",
    "(coefficients remain non-zero).\n",
    "Better for Multicollinearity: Ridge works well when there is high correlation among features (multicollinearity), as it\n",
    "distributes the coefficient values across correlated features instead of excluding them.\n",
    "\"\"\"\n",
    "Ans6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f264a4",
   "metadata": {},
   "source": [
    "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an\n",
    "example to illustrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7b41570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Regularized linear models help to prevent overfitting in machine learning by adding a penalty term to the loss function \\nthat discourages the model from assigning excessively large weights to any individual feature. This constrains the model's \\ncomplexity, making it less likely to fit noise or irrelevant patterns in the training data.\\n\\nOverfitting in Machine Learning:\\nOverfitting occurs when a model becomes too complex and starts capturing noise or random fluctuations in the training data, \\nleading to poor generalization on unseen data. In linear regression, overfitting can happen when the model has many parameters\\n(coefficients) or when the data contains outliers or irrelevant features. The model becomes too flexible, fitting the training \\ndata perfectly but failing to predict well on new data.\\n\\nHow Regularization Prevents Overfitting:\\nRegularization methods like Lasso (L1) and Ridge (L2) add a penalty for large coefficients to the loss function. This has the\\neffect of shrinking the model coefficients, reducing their values, and in the case of Lasso, possibly setting some coefficients\\nto zero. By limiting the size of the coefficients, the model is less likely to become overly complex and overfit the data.\\n\\nExample: Preventing Overfitting with Regularization:\\nScenario:\\nSuppose you’re trying to predict house prices based on several features like size, number of bedrooms, age of the house, distance from the city center, etc. You build a linear regression model and find that it performs very well on the training set but poorly on the test set — a classic case of overfitting.\\n\\nYou suspect that some features, such as the age of the house and distance from the city center, might not be as relevant or have complex relationships with the target variable. Without regularization, your model might assign large coefficients to these features, fitting the noise in the training data but failing on unseen data.\\n\\nWithout Regularization:\\nYour model’s coefficients might be very large, especially for the features that don’t significantly contribute to predicting house prices.\\nThe model fits the training data too well but generalizes poorly on new data.\\nWith Regularization (Ridge or Lasso):\\nRidge Regularization will shrink all the coefficients. This prevents any one feature from having too much influence, which might help the model generalize better.\\nLasso Regularization can shrink some coefficients to zero, automatically excluding features like age of the house or distance from the city center if they don’t add much value. This makes the model simpler and less prone to overfitting.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ans7=\"\"\"Regularized linear models help to prevent overfitting in machine learning by adding a penalty term to the loss function \n",
    "that discourages the model from assigning excessively large weights to any individual feature. This constrains the model's \n",
    "complexity, making it less likely to fit noise or irrelevant patterns in the training data.\n",
    "\n",
    "Overfitting in Machine Learning:\n",
    "Overfitting occurs when a model becomes too complex and starts capturing noise or random fluctuations in the training data, \n",
    "leading to poor generalization on unseen data. In linear regression, overfitting can happen when the model has many parameters\n",
    "(coefficients) or when the data contains outliers or irrelevant features. The model becomes too flexible, fitting the training \n",
    "data perfectly but failing to predict well on new data.\n",
    "\n",
    "How Regularization Prevents Overfitting:\n",
    "Regularization methods like Lasso (L1) and Ridge (L2) add a penalty for large coefficients to the loss function. This has the\n",
    "effect of shrinking the model coefficients, reducing their values, and in the case of Lasso, possibly setting some coefficients\n",
    "to zero. By limiting the size of the coefficients, the model is less likely to become overly complex and overfit the data.\n",
    "\n",
    "Example: Preventing Overfitting with Regularization:\n",
    "Scenario:\n",
    "Suppose you’re trying to predict house prices based on several features like size, number of bedrooms, age of the house, distance from the city center, etc. You build a linear regression model and find that it performs very well on the training set but poorly on the test set — a classic case of overfitting.\n",
    "\n",
    "You suspect that some features, such as the age of the house and distance from the city center, might not be as relevant or have complex relationships with the target variable. Without regularization, your model might assign large coefficients to these features, fitting the noise in the training data but failing on unseen data.\n",
    "\n",
    "Without Regularization:\n",
    "Your model’s coefficients might be very large, especially for the features that don’t significantly contribute to predicting house prices.\n",
    "The model fits the training data too well but generalizes poorly on new data.\n",
    "With Regularization (Ridge or Lasso):\n",
    "Ridge Regularization will shrink all the coefficients. This prevents any one feature from having too much influence, which might help the model generalize better.\n",
    "Lasso Regularization can shrink some coefficients to zero, automatically excluding features like age of the house or distance from the city center if they don’t add much value. This makes the model simpler and less prone to overfitting.\"\"\"\n",
    "Ans7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49b5e09",
   "metadata": {},
   "source": [
    "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best\n",
    "choice for regression analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65e4bf67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'While regularized linear models (such as Lasso and Ridge) are effective tools for preventing overfitting and handling high-dimensional data, they have several limitations that may make them less suitable in certain scenarios. Below are some key limitations of regularized linear models, along with explanations of when they may not be the best choice for regression analysis.\\n\\n1. Assumption of Linearity\\nLimitation: Regularized linear models, like ordinary least squares (OLS) regression, assume a linear relationship between the features (independent variables) and the target (dependent variable). In cases where the true relationship is non-linear, these models can struggle to capture the underlying patterns, even with regularization applied.\\n\\nWhy It’s a Problem: Many real-world problems exhibit complex, non-linear relationships between variables. In such cases, linear models may underperform compared to non-linear models like decision trees, random forests, or neural networks.\\n\\nExample: If you\\'re predicting the effect of dosage on drug efficacy, where the relationship might be non-linear (e.g., efficacy increases rapidly up to a certain point and then levels off), a linear model won’t capture this pattern well, even if regularization is applied.\\n\\n2. Feature Engineering Requirement\\nLimitation: Regularized linear models still depend heavily on the quality of the features provided. If important non-linear interactions or transformations of the features are not included, the model may miss critical information, even if regularized.\\n\\nWhy It’s a Problem: Unlike more complex models, such as tree-based algorithms or neural networks, regularized linear models do not automatically discover complex interactions between features. As a result, feature engineering (e.g., creating interaction terms, polynomial features) is often required to achieve good performance.\\n\\nExample: In predicting housing prices, regularized linear models won’t automatically detect non-linear relationships, like the fact that very large houses or houses located extremely far from the city center may follow different price patterns. Without explicit feature engineering, performance may suffer.\\n\\n3. Poor Handling of Categorical Variables\\nLimitation: Regularized linear models typically require one-hot encoding or some other transformation for categorical variables, which can lead to a significant increase in the number of features. This increases the dimensionality of the problem and may reduce the effectiveness of the regularization.\\n\\nWhy It’s a Problem: One-hot encoding for high-cardinality categorical variables can lead to a very large feature space, making the model harder to interpret and potentially leading to overfitting despite regularization.\\n\\nExample: In a dataset with a categorical feature like \"zip code\" with hundreds of unique values, the one-hot encoding will create hundreds of additional features. This can make the regularized linear model less efficient compared to algorithms that can handle categorical variables more naturally, like decision trees or gradient boosting.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ans8=\"\"\"While regularized linear models (such as Lasso and Ridge) are effective tools for preventing overfitting and handling high-dimensional data, they have several limitations that may make them less suitable in certain scenarios. Below are some key limitations of regularized linear models, along with explanations of when they may not be the best choice for regression analysis.\n",
    "\n",
    "1. Assumption of Linearity\n",
    "Limitation: Regularized linear models, like ordinary least squares (OLS) regression, assume a linear relationship between the features (independent variables) and the target (dependent variable). In cases where the true relationship is non-linear, these models can struggle to capture the underlying patterns, even with regularization applied.\n",
    "\n",
    "Why It’s a Problem: Many real-world problems exhibit complex, non-linear relationships between variables. In such cases, linear models may underperform compared to non-linear models like decision trees, random forests, or neural networks.\n",
    "\n",
    "Example: If you're predicting the effect of dosage on drug efficacy, where the relationship might be non-linear (e.g., efficacy increases rapidly up to a certain point and then levels off), a linear model won’t capture this pattern well, even if regularization is applied.\n",
    "\n",
    "2. Feature Engineering Requirement\n",
    "Limitation: Regularized linear models still depend heavily on the quality of the features provided. If important non-linear interactions or transformations of the features are not included, the model may miss critical information, even if regularized.\n",
    "\n",
    "Why It’s a Problem: Unlike more complex models, such as tree-based algorithms or neural networks, regularized linear models do not automatically discover complex interactions between features. As a result, feature engineering (e.g., creating interaction terms, polynomial features) is often required to achieve good performance.\n",
    "\n",
    "Example: In predicting housing prices, regularized linear models won’t automatically detect non-linear relationships, like the fact that very large houses or houses located extremely far from the city center may follow different price patterns. Without explicit feature engineering, performance may suffer.\n",
    "\n",
    "3. Poor Handling of Categorical Variables\n",
    "Limitation: Regularized linear models typically require one-hot encoding or some other transformation for categorical variables, which can lead to a significant increase in the number of features. This increases the dimensionality of the problem and may reduce the effectiveness of the regularization.\n",
    "\n",
    "Why It’s a Problem: One-hot encoding for high-cardinality categorical variables can lead to a very large feature space, making the model harder to interpret and potentially leading to overfitting despite regularization.\n",
    "\n",
    "Example: In a dataset with a categorical feature like \"zip code\" with hundreds of unique values, the one-hot encoding will create hundreds of additional features. This can make the regularized linear model less efficient compared to algorithms that can handle categorical variables more naturally, like decision trees or gradient boosting.\"\"\"\n",
    "Ans8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fb9991",
   "metadata": {},
   "source": [
    "Q9. You are comparing the performance of two regression models using different evaluation metrics.\n",
    "Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
    "performer, and why? Are there any limitations to your choice of metric?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f63ab516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Choosing the Better Model\\nComparison:\\nModel A: RMSE = 10\\nModel B: MAE = 8\\nSince Model B has a lower MAE than Model A’s RMSE, it means Model B, on average, makes smaller errors. However, the direct comparison between RMSE and MAE can be misleading because they measure different aspects of model performance:\\n\\nSensitivity to Large Errors: RMSE is more sensitive to large errors because it squares the differences. If the cost of large errors is high in your application, RMSE may be more relevant.\\n\\nOverall Error Magnitude: MAE provides a more straightforward measure of average error magnitude. If you’re more concerned with the typical size of errors without worrying about outliers or large errors specifically, MAE might be more informative.\\n\\nDecision Factors:\\nCost of Large Errors: If large errors have a disproportionately high cost or impact in your application (e.g., financial forecasts or safety-critical systems), RMSE might be more appropriate to minimize.\\n\\nGeneral Error Size: If your primary concern is minimizing average error without putting extra weight on outliers, MAE might be preferable.\\n\\nLimitations of Your Choice of Metric\\nDifferent Sensitivities: RMSE and MAE provide different perspectives on model performance. RMSE can be higher due to outliers or large errors, while MAE is less affected by extreme values. This means that choosing one metric over the other might miss important details about model performance.\\n\\nContext-Specific Importance: The choice of metric should align with the specific goals and context of your application. For instance, in some scenarios, reducing the maximum possible error (which RMSE is more sensitive to) may be more crucial than reducing the average error.\\n\\nAdditional Metrics: Relying solely on RMSE or MAE may not provide a complete picture. Considering additional metrics like R-squared, Mean Absolute Percentage Error (MAPE), or even cross-validation results could provide a more comprehensive evaluation of model performance.\\n\\nData Distribution: If the data contains significant outliers or the error distribution is skewed, relying on a single metric might not fully capture the model's effectiveness.\\n\\nConclusion\\nIn general, if Model B’s MAE is significantly lower and you don’t have a specific concern about large errors, you might prefer Model B for its overall lower average error. However, if large errors are particularly costly or problematic in your application, you should consider RMSE or other metrics that align with your specific needs. Always consider the context and implications of the metrics you choose to ensure they align with your goals and the nature of your data.\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ans9=\"\"\"Choosing the Better Model\n",
    "Comparison:\n",
    "Model A: RMSE = 10\n",
    "Model B: MAE = 8\n",
    "Since Model B has a lower MAE than Model A’s RMSE, it means Model B, on average, makes smaller errors. However, the direct comparison between RMSE and MAE can be misleading because they measure different aspects of model performance:\n",
    "\n",
    "Sensitivity to Large Errors: RMSE is more sensitive to large errors because it squares the differences. If the cost of large errors is high in your application, RMSE may be more relevant.\n",
    "\n",
    "Overall Error Magnitude: MAE provides a more straightforward measure of average error magnitude. If you’re more concerned with the typical size of errors without worrying about outliers or large errors specifically, MAE might be more informative.\n",
    "\n",
    "Decision Factors:\n",
    "Cost of Large Errors: If large errors have a disproportionately high cost or impact in your application (e.g., financial forecasts or safety-critical systems), RMSE might be more appropriate to minimize.\n",
    "\n",
    "General Error Size: If your primary concern is minimizing average error without putting extra weight on outliers, MAE might be preferable.\n",
    "\n",
    "Limitations of Your Choice of Metric\n",
    "Different Sensitivities: RMSE and MAE provide different perspectives on model performance. RMSE can be higher due to outliers or large errors, while MAE is less affected by extreme values. This means that choosing one metric over the other might miss important details about model performance.\n",
    "\n",
    "Context-Specific Importance: The choice of metric should align with the specific goals and context of your application. For instance, in some scenarios, reducing the maximum possible error (which RMSE is more sensitive to) may be more crucial than reducing the average error.\n",
    "\n",
    "Additional Metrics: Relying solely on RMSE or MAE may not provide a complete picture. Considering additional metrics like R-squared, Mean Absolute Percentage Error (MAPE), or even cross-validation results could provide a more comprehensive evaluation of model performance.\n",
    "\n",
    "Data Distribution: If the data contains significant outliers or the error distribution is skewed, relying on a single metric might not fully capture the model's effectiveness.\n",
    "\n",
    "Conclusion\n",
    "In general, if Model B’s MAE is significantly lower and you don’t have a specific concern about large errors, you might prefer Model B for its overall lower average error. However, if large errors are particularly costly or problematic in your application, you should consider RMSE or other metrics that align with your specific needs. Always consider the context and implications of the metrics you choose to ensure they align with your goals and the nature of your data.\n",
    "\"\"\"\n",
    "Ans9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba2a693",
   "metadata": {},
   "source": [
    "Q10. You are comparing the performance of two regularized linear models using different types of\n",
    "regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
    "uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\n",
    "better performer, and why? Are there any trade-offs or limitations to your choice of regularization\n",
    "method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b62365a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'When comparing two regularized linear models—one using Ridge regularization and the other using Lasso regularization—several factors need to be considered to determine which model is better. Here’s a detailed analysis to help you decide:\\n\\nComparison of Models\\nModel A (Ridge Regularization):\\n\\nRegularization Parameter: 0.1\\nRegularization Type: L2 regularization\\nEffect: Ridge regularization adds a penalty proportional to the square of the magnitude of coefficients, which helps to prevent overfitting by shrinking coefficients but does not set them to zero.\\nModel B (Lasso Regularization):\\n\\nRegularization Parameter: 0.5\\nRegularization Type: L1 regularization\\nEffect: Lasso regularization adds a penalty proportional to the absolute value of the coefficients, which can shrink some coefficients to zero, performing feature selection.\\nChoosing the Better Model\\nFactors to Consider:\\nFeature Selection vs. Shrinking Coefficients:\\n\\nLasso (Model B): Lasso can drive some coefficients to zero, effectively performing feature selection. This is useful if you suspect that many features are irrelevant or if you want a more interpretable model with fewer features.\\nRidge (Model A): Ridge regularization shrinks all coefficients but does not perform feature selection. It’s beneficial when all features are believed to have some relevance or when multicollinearity is a concern.\\nRegularization Parameter:\\n\\nMagnitude of Regularization: The value of the regularization parameter (\\n𝜆\\nλ) influences the strength of the penalty:\\nModel A (0.1): This relatively low value suggests a smaller penalty, which means the model is less regularized and may fit the training data more closely.\\nModel B (0.5): This higher value indicates a stronger penalty, which means the model is more regularized and will have more coefficients shrunk or set to zero.\\nImpact on Performance: Higher regularization might improve generalization if the model is prone to overfitting, but it might also lead to underfitting if the regularization is too strong.\\nModel Complexity and Interpretability:\\n\\nModel B (Lasso): With Lasso, the feature selection process can simplify the model, potentially improving interpretability by focusing only on important features.\\nModel A (Ridge): Ridge may retain all features but with smaller coefficients, making it less interpretable if the number of features is large.\\nPerformance Metrics:\\n\\nEvaluation Metrics: To determine which model performs better, you should evaluate both models using metrics like RMSE, MAE, R-squared, or cross-validation performance on a validation set or test set.\\nApplication Context:\\n\\nIf Feature Selection is Important: Choose Lasso (Model B) if you need to identify and select a subset of important features.\\nIf Handling Multicollinearity is Critical: Choose Ridge (Model A) if multicollinearity is an issue and you want to retain all features.\\nTrade-offs and Limitations\\nRidge Regularization (Model A):\\nAdvantages:\\nHandles multicollinearity by shrinking coefficients.\\nRetains all features, which might be desirable if all features are relevant.\\nLimitations:\\nDoes not perform feature selection, so the model might be less interpretable with many features.\\nMay not be as effective if many features are truly irrelevant.\\nLasso Regularization (Model B):\\nAdvantages:\\nPerforms automatic feature selection by setting some coefficients to zero, leading to simpler, more interpretable models.\\nCan be beneficial when you suspect that many features are not useful.\\nLimitations:\\nMay be unstable when features are highly correlated, as it might arbitrarily select one feature from a group of correlated features while excluding others.\\nStrong regularization (high \\n𝜆\\nλ) might lead to underfitting by excluding too many features.\\nSummary\\nChoose Model A (Ridge) if:\\n\\nMulticollinearity is a concern.\\nYou prefer to keep all features in the model.\\nThe regularization parameter (0.1) is providing a good balance between underfitting and overfitting, based on your validation metrics.\\nChoose Model B (Lasso) if:\\n\\nFeature selection is important, and you want a more interpretable model.\\nThe regularization parameter (0.5) helps to effectively remove irrelevant features without causing underfitting.\\nConsider Both: Use cross-validation to compare the performance of both models on your specific dataset. The choice between Lasso and Ridge might also depend on the particular application and the impact of feature selection versus coefficient shrinkage on model performance.\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ans10=\"\"\"When comparing two regularized linear models—one using Ridge regularization and the other using Lasso regularization—several factors need to be considered to determine which model is better. Here’s a detailed analysis to help you decide:\n",
    "\n",
    "Comparison of Models\n",
    "Model A (Ridge Regularization):\n",
    "\n",
    "Regularization Parameter: 0.1\n",
    "Regularization Type: L2 regularization\n",
    "Effect: Ridge regularization adds a penalty proportional to the square of the magnitude of coefficients, which helps to prevent overfitting by shrinking coefficients but does not set them to zero.\n",
    "Model B (Lasso Regularization):\n",
    "\n",
    "Regularization Parameter: 0.5\n",
    "Regularization Type: L1 regularization\n",
    "Effect: Lasso regularization adds a penalty proportional to the absolute value of the coefficients, which can shrink some coefficients to zero, performing feature selection.\n",
    "Choosing the Better Model\n",
    "Factors to Consider:\n",
    "Feature Selection vs. Shrinking Coefficients:\n",
    "\n",
    "Lasso (Model B): Lasso can drive some coefficients to zero, effectively performing feature selection. This is useful if you suspect that many features are irrelevant or if you want a more interpretable model with fewer features.\n",
    "Ridge (Model A): Ridge regularization shrinks all coefficients but does not perform feature selection. It’s beneficial when all features are believed to have some relevance or when multicollinearity is a concern.\n",
    "Regularization Parameter:\n",
    "\n",
    "Magnitude of Regularization: The value of the regularization parameter (\n",
    "𝜆\n",
    "λ) influences the strength of the penalty:\n",
    "Model A (0.1): This relatively low value suggests a smaller penalty, which means the model is less regularized and may fit the training data more closely.\n",
    "Model B (0.5): This higher value indicates a stronger penalty, which means the model is more regularized and will have more coefficients shrunk or set to zero.\n",
    "Impact on Performance: Higher regularization might improve generalization if the model is prone to overfitting, but it might also lead to underfitting if the regularization is too strong.\n",
    "Model Complexity and Interpretability:\n",
    "\n",
    "Model B (Lasso): With Lasso, the feature selection process can simplify the model, potentially improving interpretability by focusing only on important features.\n",
    "Model A (Ridge): Ridge may retain all features but with smaller coefficients, making it less interpretable if the number of features is large.\n",
    "Performance Metrics:\n",
    "\n",
    "Evaluation Metrics: To determine which model performs better, you should evaluate both models using metrics like RMSE, MAE, R-squared, or cross-validation performance on a validation set or test set.\n",
    "Application Context:\n",
    "\n",
    "If Feature Selection is Important: Choose Lasso (Model B) if you need to identify and select a subset of important features.\n",
    "If Handling Multicollinearity is Critical: Choose Ridge (Model A) if multicollinearity is an issue and you want to retain all features.\n",
    "Trade-offs and Limitations\n",
    "Ridge Regularization (Model A):\n",
    "Advantages:\n",
    "Handles multicollinearity by shrinking coefficients.\n",
    "Retains all features, which might be desirable if all features are relevant.\n",
    "Limitations:\n",
    "Does not perform feature selection, so the model might be less interpretable with many features.\n",
    "May not be as effective if many features are truly irrelevant.\n",
    "Lasso Regularization (Model B):\n",
    "Advantages:\n",
    "Performs automatic feature selection by setting some coefficients to zero, leading to simpler, more interpretable models.\n",
    "Can be beneficial when you suspect that many features are not useful.\n",
    "Limitations:\n",
    "May be unstable when features are highly correlated, as it might arbitrarily select one feature from a group of correlated features while excluding others.\n",
    "Strong regularization (high \n",
    "𝜆\n",
    "λ) might lead to underfitting by excluding too many features.\n",
    "Summary\n",
    "Choose Model A (Ridge) if:\n",
    "\n",
    "Multicollinearity is a concern.\n",
    "You prefer to keep all features in the model.\n",
    "The regularization parameter (0.1) is providing a good balance between underfitting and overfitting, based on your validation metrics.\n",
    "Choose Model B (Lasso) if:\n",
    "\n",
    "Feature selection is important, and you want a more interpretable model.\n",
    "The regularization parameter (0.5) helps to effectively remove irrelevant features without causing underfitting.\n",
    "Consider Both: Use cross-validation to compare the performance of both models on your specific dataset. The choice between Lasso and Ridge might also depend on the particular application and the impact of feature selection versus coefficient shrinkage on model performance.\n",
    "\"\"\"\n",
    "Ans10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd355f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
